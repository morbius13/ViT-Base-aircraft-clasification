{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4sWIAl7cOUO"
      },
      "outputs": [],
      "source": [
        "# upload your kaggle.json before running\n",
        "# install dependencies, download and unzip the dataset\n",
        "!pip install torchinfo\n",
        "!pip install torchmetrics\n",
        "!pip install kaggle\n",
        "!pip install efficientnet_pytorch\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d a2015003713/militaryaircraftdetectiondataset\n",
        "!unzip militaryaircraftdetectiondataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "rY4cwB63iaDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualise random image from dataset\n",
        "import random\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "\n",
        "data_path = Path(\"crop/\")\n",
        "\n",
        "image_path_list = list(data_path.glob(\"*/*.jpg\"))\n",
        "\n",
        "random_image_path = random.choice(image_path_list)\n",
        "\n",
        "image_class = random_image_path.parent.stem\n",
        "\n",
        "img = Image.open(random_image_path)\n",
        "\n",
        "print(f\"Random image path: {random_image_path}\")\n",
        "print(f\"Image class: {image_class}\")\n",
        "print(f\"Image height: {img.height}\")\n",
        "print(f\"Image width: {img.width}\")\n",
        "img"
      ],
      "metadata": {
        "id": "X3ZkmFVPifL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the data into train and test\n",
        "from typing_extensions import TypeVarTuple\n",
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class_names = []\n",
        "\n",
        "data_path = Path(\"crop\")\n",
        "test_path = Path(\"test\")\n",
        "train_path = Path(\"train\")\n",
        "\n",
        "os.makedirs(train_path, exist_ok=True)\n",
        "os.makedirs(test_path, exist_ok=True)\n",
        "\n",
        "for item in os.listdir(data_path):\n",
        "  class_names.append(item)\n",
        "\n",
        "for class_name in class_names:\n",
        "  os.makedirs(train_path/class_name, exist_ok=True)\n",
        "  os.makedirs(test_path/class_name, exist_ok=True)\n",
        "\n",
        "  images = [image for image in os.listdir(data_path/class_name) if image.endswith(\".jpg\")]\n",
        "\n",
        "  train_images, test_images = train_test_split(images, test_size=0.2)\n",
        "\n",
        "  for train_image in train_images:\n",
        "    src_path = data_path/class_name/train_image\n",
        "    dest_path = train_path/class_name\n",
        "    shutil.move(src_path, dest_path)\n",
        "\n",
        "  for test_image in test_images:\n",
        "    src_path = data_path/class_name/test_image\n",
        "    dest_path = test_path/class_name\n",
        "    shutil.move(src_path, dest_path)"
      ],
      "metadata": {
        "id": "clS1zkvPqu0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "IMSIZE = 224\n",
        "\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.Resize(size=(IMSIZE, IMSIZE)),\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "oiKYdSgisQ6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_data = ImageFolder(root=train_path,\n",
        "                         transform=data_transform,\n",
        "                         target_transform=None)\n",
        "\n",
        "test_data = ImageFolder(root=test_path,\n",
        "                        transform=data_transform,\n",
        "                        target_transform=None)\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_dataloader = DataLoader(dataset=train_data,\n",
        "                              batch_size=BATCH_SIZE,\n",
        "                              shuffle=True,\n",
        "                              num_workers=os.cpu_count())\n",
        "\n",
        "test_dataloader = DataLoader(dataset=test_data,\n",
        "                             batch_size=BATCH_SIZE,\n",
        "                             shuffle=False,\n",
        "                             num_workers=os.cpu_count())\n",
        "\n",
        "class_labels = train_data.classes\n",
        "num_classes = len(class_labels)"
      ],
      "metadata": {
        "id": "KWfuImtWZkY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ViT Equation 1**\n",
        "\n",
        "$\\bf{z_0} = [x_{class}; x^{1}_pE; x^{2}_pE;...;x^{N}_pE] + E_{pos}$\n",
        "\n",
        "$E \\in \\mathbb{R}^{(P^{2}\\cdot C)\\times D}$   \n",
        "\n",
        "$E_{pos} \\in \\mathbb{R}^{(N+1)\\times D}$\n",
        "\n",
        "Reshape the image $x \\in \\mathbb{R}^{H \\times W \\times C}$ where H is the height of the image, W is the width and C is the number of color channels into a sequence of flattened 2D patches $\\bf x_p \\in \\mathbb{R}^{N \\times (P^{2} \\cdot C)}$ where C is the number of color channels, P is the resolution of each image patch and $\\bf N = \\frac{HW}{P^{2}}$ is the resulting number of patches. We flatten the patches and map to D dimensions with a trainable linear projection (In ViT-Base case D = 768 and P = 16).\n",
        "\n",
        "```x_input = [class_token, patch_1, patch_2, ...] + [class_token_position, patch_1_position, patch_2_position,...]```"
      ],
      "metadata": {
        "id": "pw1F1qx35tfh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ViT Equation 2 and 3**\n",
        "\n",
        "Eq. 2\n",
        "\n",
        "$\\bf z'_\\iota = MSA(LN(z_{\\iota-1})) + z_{\\iota-1}$\n",
        "\n",
        "$\\iota = 1...L$\n",
        "\n",
        "From every layer from 1 to $L$ number of layers there is a $\\bf LN$ layer (linear norm layer) wrapped in $\\bf MSA$ (Multi-Head Attention) layer\n",
        "\n",
        "Eq.3\n",
        "\n",
        "$\\bf z_{\\iota} = MLP(LN(z'_{\\iota}))+z'_{\\iota}$\n",
        "\n",
        "From every layer from 1 to $L$ number of layers there is a $\\bf LN$ layer (linear norm layer) wrapped in $\\bf MLP$ (Multi-Layer Perceptron) layer"
      ],
      "metadata": {
        "id": "q5GdPo-a_sXo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**ViT Equation 4**\n",
        "\n",
        "$\\bf y = LN(z^{0}_L)$\n",
        "\n",
        "For the last layer $L$ the output $y$ is the zero index token of $\\bf z$ wrapped in LayerNorm $\\bf(LN)$ layer"
      ],
      "metadata": {
        "id": "8OML8MSHCq0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "H = 224 # height\n",
        "W = 224 # width\n",
        "C = 3   # color channels\n",
        "P = 16  # patch size\n",
        "D = 768 # hidden units\n",
        "N = int((H*W)/P**2) # number of patches\n",
        "N"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5sDPNnxFldy",
        "outputId": "6c792d49-199a-4e17-d3cd-ab7594918505"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "196"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Input image shape: {H}x{W}x{C}\")\n",
        "print(f\"Output shape of flattened 2D patches: {N}x{(P**2)*C}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFqROy7XGzCp",
        "outputId": "e4e3663b-a058-49b0-a295-cd609609bb03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input image shape: 224x224x3\n",
            "Output shape of flattened 2D patches: 196x768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "  def __init__(self,\n",
        "               in_channels: int=C,\n",
        "               embedding_dim: int=D,\n",
        "               patch_size: int=P):\n",
        "    super().__init__()\n",
        "\n",
        "    self.patcher = nn.Conv2d(in_channels=in_channels,\n",
        "                             out_channels=embedding_dim,\n",
        "                             kernel_size=patch_size,\n",
        "                             stride=patch_size)\n",
        "\n",
        "    self.flattener = nn.Flatten(start_dim=2,\n",
        "                                end_dim=3)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.flattener(self.patcher(x))\n",
        "    return x.permute(0, 2, 1)"
      ],
      "metadata": {
        "id": "I9G3MVftQPQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MSABlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                 embedding_dim: int=D,\n",
        "                 num_heads:int=12,\n",
        "                 dropout:float=0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
        "\n",
        "        self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
        "                                                    num_heads=num_heads,\n",
        "                                                    dropout=dropout,\n",
        "                                                    batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_norm(x)\n",
        "        attn_output, _ = self.multihead_attn(query=x, key=x, value=x, need_weights=False)\n",
        "        return attn_output"
      ],
      "metadata": {
        "id": "bUj5bCFQ-te4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                 embedding_dim: int=D,\n",
        "                 mlp_size: int=3072,\n",
        "                 dropout: float=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(in_features=embedding_dim, out_features=mlp_size),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(in_features=mlp_size, out_features=embedding_dim),\n",
        "            nn.Dropout(p=dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_norm(x)\n",
        "        x = self.mlp(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "MW2YESN_BKTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                 embedding_dim: int=D,\n",
        "                 num_heads: int=12,\n",
        "                 mlp_size: int=3072,\n",
        "                 mlp_dropout: float=0.1,\n",
        "                 attn_dropout: float=0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.msa_block = MSABlock(embedding_dim=embedding_dim,\n",
        "                                                     num_heads=num_heads,\n",
        "                                                     dropout=attn_dropout)\n",
        "\n",
        "        self.mlp_block =  MLPBlock(embedding_dim=embedding_dim,\n",
        "                                   mlp_size=mlp_size,\n",
        "                                   dropout=mlp_dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x =  self.msa_block(x) + x\n",
        "        x = self.mlp_block(x) + x\n",
        "        return x"
      ],
      "metadata": {
        "id": "LaQ1YrvcC_1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(nn.Module):\n",
        "  def __init__(self,\n",
        "               img_size: int=IMSIZE,\n",
        "               in_channels: int=C,\n",
        "               patch_size: int=P,\n",
        "               num_transformer_layers: int=12,\n",
        "               embedding_dim: int=D,\n",
        "               mlp_size: int=3072,\n",
        "               num_heads: int=12,\n",
        "               attn_dropout: float=0,\n",
        "               mlp_dropout: float=0.1,\n",
        "               embedding_dropout: float=0.1,\n",
        "               num_classes: int=num_classes):\n",
        "    super().__init__()\n",
        "\n",
        "    assert img_size % patch_size == 0, f\"Image size must be divisable by patch size\"\n",
        "\n",
        "    self.num_patches = (img_size * img_size) // patch_size**2\n",
        "\n",
        "    self.class_token = nn.Parameter(torch.randn(1, self.num_patches+1, embedding_dim), requires_grad=True)\n",
        "\n",
        "    self.position_embedding = nn.Parameter(torch.randn(1, 1, embedding_dim), requires_grad=True)\n",
        "\n",
        "    self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n",
        "\n",
        "    self.patch_embedding = PatchEmbedding(in_channels=in_channels, patch_size=patch_size, embedding_dim=embedding_dim)\n",
        "\n",
        "    self.transformer_encoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim=embedding_dim,\n",
        "                                                                       num_heads=num_heads,\n",
        "                                                                       mlp_size=mlp_size,\n",
        "                                                                       mlp_dropout=mlp_dropout) for _ in range(num_transformer_layers)])\n",
        "\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.LayerNorm(normalized_shape=embedding_dim),\n",
        "        nn.Linear(in_features=embedding_dim, out_features=num_classes)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    batch_size = x.shape[0]\n",
        "\n",
        "    class_token = self.class_token.expand(batch_size, -1, -1)\n",
        "\n",
        "    x = self.patch_embedding(x)\n",
        "\n",
        "    x = torch.cat((class_token, x), dim=1)\n",
        "\n",
        "    x = self.position_embedding + x\n",
        "\n",
        "    x = self.embedding_dropout(x)\n",
        "\n",
        "    x = self.transformer_encoder(x)\n",
        "\n",
        "    x = self.classifier(x[:, 0])\n",
        "\n",
        "    return  x"
      ],
      "metadata": {
        "id": "TOcHOuFSJmUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit = ViT(num_classes=len(class_names)).to(device)"
      ],
      "metadata": {
        "id": "1M9ySdpJSiGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "summary(model=vit,\n",
        "        input_size=(BATCH_SIZE, C, H, W),\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"]\n",
        ")"
      ],
      "metadata": {
        "id": "0FkqE_ZFM4vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=vit.parameters(), lr=3e-3, betas=(0.9, 0.999), weight_decay=0.3)"
      ],
      "metadata": {
        "id": "_TahovXPWeXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "from torchmetrics import Accuracy\n",
        "\n",
        "accuracy = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
        "\n",
        "EPOCHS = 10\n",
        "\n",
        "for epoch in tqdm(range(EPOCHS)):\n",
        "  vit.train()\n",
        "\n",
        "  train_loss, train_acc = 0, 0\n",
        "\n",
        "  for X, y in train_dataloader:\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    pred = vit(X)\n",
        "\n",
        "    loss = loss_fn(pred, y)\n",
        "    train_loss += loss.item()\n",
        "    train_acc += accuracy(pred, y)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "  train_loss /= len(train_dataloader)\n",
        "  train_acc /= len(train_dataloader)\n",
        "\n",
        "\n",
        "  test_loss = 0\n",
        "  test_acc = 0\n",
        "\n",
        "  vit.eval()\n",
        "  with torch.inference_mode():\n",
        "    for X, y in test_dataloader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      test_pred = vit(X)\n",
        "\n",
        "      loss = loss_fn(test_pred, y)\n",
        "      test_loss += loss.item()\n",
        "      test_acc += accuracy(test_pred, y)\n",
        "\n",
        "  test_loss /= len(test_dataloader)\n",
        "  test_acc /= len(test_dataloader)\n",
        "\n",
        "  print(f\"Train loss: {train_loss:.4f} | Train acc: {train_acc:.2f} | Test loss: {test_loss:.4f} | Test acc: {test_acc:.2f}\")"
      ],
      "metadata": {
        "id": "_zU-T1NIXHl2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}